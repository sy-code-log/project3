## 🗽 뉴욕 에어비앤비 숙소 가격 예측 & 비교  
**프로젝트명: _호갱탈출 프로젝트_**

15개의 핵심 변수를 활용하여 **숙소 가격을 예측**하고,  
여행자는 실제 숙소 정보를 입력하여 실제 가격에 비해 **과도한 요금인지, 적정한지** 비교 가능 
이 시스템을 통해 여행자는 **‘호갱’을 피하고**, **숙소 선택의 기준을 더욱 명확히** 할 수 있음

---

## 🔎 프로젝트 배경

1. **단기 숙소 시장의 혼란과 불안정성**
- 단기 임대 주택이 크게 증가
- 거주용 주택이 줄어 임대료 9.2% 상승
- 이에 따라 **단기 임대 규제 강화**
- 출처: Bloomberg(2024.07.09)
<br>

2. **가격 급등과 변동성 심화**
- 규제 이후, 숙박시설 수가 약 89% 급감
- 가격 급등과 변동성 심화 -> **가격 편차 확대**
- 출처:비즈트리뷴(2024.07.03)
<br>

3. **스마트 요금제와 신뢰도 하락**
- 자동화된 스마트 요금제 도입 이후 소비자와 호스트 모두 **가격의 기준과 적정성을 판단하기 어려움**
- 출처: 이코노미스트, 〈뉴욕, 에어비앤비 숙소 89% 사라졌다〉(2021.08.17)

---

## 🎯 프로젝트 목표

1. **가격 결정 요인 분석**
- 가격 결정에 영향을 주는 **핵심 변수**들을 파악하고,
- 이를 바탕으로 합리적인 가격 설정 전략을 세울 수 있도록 분석
<br>

2. **합리적 숙박비 범위 제시**
- 분석 결과를 바탕으로 **가격 예측 모델을 구축**하여,
- 예상 숙박비와 오차의 범위를 사용자에게 제시
<br>

3. 적정가 판별 서비스 개발
- 사용자는 **직접 숙소 정보를 입력**하고,
- 자신이 선택한 숙소의 **가격이 적정한지 판단할 수 있는 서비스**를 경험
<br>

> 최종 목표 : 가격의 불확실성을 해소하여, 소비자가 더 합리적인 선택을 할 수 있도록 돕는 것

---

## 📊 데이터 설명

DATA 1. 에어비앤비 웹 크롤링 데이터 (2025-03-02 기준)
- 캐글에 게시된 에어비앤비 숙소 가격 데이터
- 총 72개 컬럼 중, 분석 목적에 맞춰 주요 변수 15개 선정
- 숙소 정보 | `Room Type`, `Bedrooms`, `Bathrooms`, `Beds`, `Amenities`
- 위치 정보 | `Longitude`, `Latitude`, `Neighbourhood`, `Neighbourhood Group`
- 숙박 조건 | `Accommodates`, `Minimum Nights`, `Maximum Nights`, `Instant Bookable`
- 평가 및 가격 | `Review Scores Rating`, `Price`
<br>

DATA 2. MAPBOX 데이터
- 미국 정부의 공공 데이터 MTA(메트로폴리탄 교통국)의 GTFS(General Transit Feed Specification)
- 각 숙소에서 근접한 위치의 승차장들과 공항과의 거리

---

## 🛠️ 분석 도구

- 전처리, 연산 : python, pandas, numpy
- 지도 : mapbox, folium, geopandas, geojson
- 시각화 : seaborn, matplotlib
- 머신러닝 : sklearn, catboost, xgboost, optuna
- 서비스 개발, 배포 : streamlit, spaces, git

---

## 🧹 데이터 전처리

- 전체 72개 컬럼 중 가격에 영향이 있을 것이라 판단한 15개 컬럼 선정 및 전처리
- 전처리 : 소문자 통일, 공백제거, 수치형 변수 문자열 제거($, 등)
- 이상치 & 결측치 : 이상치 제거, 수치형 변수 결측치 중앙값 or 평균 대체, 범주형 변수 'MISSING'

--- 

## 🤖 모델 성능 비교 테스트

| 모델                           | 데이터 | 📏 MAE | 📉 MAPE | 📐 RMSE | 📊 R² Score |
|--------------------------------|----------|--------|---------|--------|----------------|
| 🌳 Random Forest               | Train  | 15.03 | 9.57% | 24.72 | 0.9364 |
| 🌳 Random Forest               | Test   | 37.89 | 26.00% | 56.53 | 0.6729 |
| 📉 Linear Regression           | Train  | 55.43 | 40.35% | 76.89 | 0.3844 |
| 📉 Linear Regression           | Test   | 55.55 | 39.08% | 77.64 | 0.3830 |
| 🌱 Gradient Boosting Regressor | Train  | 42.85 | 30.19% | 61.59 | 0.6050 |
| 🌱 Gradient Boosting Regressor | Test   | 43.29 | 30.11% | 62.20 | 0.6040 |
| 🧠 XGBoost                     | Train  | 30.30 | 20.21% | 45.66 | 0.7829 |
| 🧠 XGBoost                     | Test   | 39.97 | 27.26% | 57.89 | 0.6570 |
| 🐱 CatBoost                    | Train  | 36.89 | 25.41% | 56.01 | 0.6734 |
| 🐱 CatBoost                    | Test   | 40.35 | 27.78% | 58.92 | 0.6447 |

---

## 🏆 모델 기본 성능 비교 테스트 결과

_최종 모델 선정 : **CATBOOST**_

1. **과적합에 강한 성능**
- CATBOOST는 R2 성능도 준수하고 훈련/검증 성능 간 차이가 적은 모델
- 이는 데이터의 노이즈나 복잡성에 대해 안정적이라는 의미로,
- 실제 고객 데이터에 적용 시 신뢰도 높은 예측 가능
<br>

2. **추후 사용할 변수(`amenities`) 처리에 강점**
- 일반 ML 모델은 텍스트 데이터를 숫자형으로 변환 사용 가능
- CATBOOST는 TEXT_FEATURES 파라미터로 텍스트 변수도 직접 입력 가능
- 우리 데이터의 핵심 변수 중 하나인 `amenities`는 리스트 형태의 비정형 텍스트 데이터
- 이를 간단히 전처리 후에 자연스럽게 활용 가능한 모델이 **CATBOOST**

---

## 💵 모델 성능 향상 전처리 1 _ `PRICE` 기준 재설정
> 가격 상한, 하한 재설정 후 모델 안정화를 위한 로그 적용
<br>

1. **하한선**
- 에어비앤비 내 뉴욕 숙소 최저 가격 약 ₩50000 (2025-07-24 기준)
- 달러 환산시 약 $37
<br>

2. **상한선**
- 시장 기준 | $400 초반
- 실험 구간 | $400 ~ $480
- 최종 선택 | IQR 1.5 (≈472$)
- 이유 | 시장반영 + 성능 최적
<br>

⭐️ 전처리 1 이후 성능 평가 결과

| 단계       | 📏 MAE    | 📉 MAPE   | 📐 RMSE     | 📊 R² Score |
|------------|-----------|-----------|-------------|-------------|
| ⚙️ 기본 모델     | **70.595**    | 28.20%    | 311.9109   | 0.2826      |
| 🧹 전처리 01 적용 | **37.0532** | 24.69% | 53.8511 | 0.7133  |

--- 

## 📝 모델 성능 향상 전처리 2 _ `amenities` 텍스트 전처리
> catboost 모델에서 text_features로 사용하기 위한 컬럼 변환
<br>

1. 기존 `amenities`
- 문자열 내 중복키워드와 이스케이프 문자 포함
- 중복 키워드 병합 필요
- 이후 CATBOOST가 처리할 수 있도록 텍스트 형태로 변환하여 모델에 활용 예정
<br>

2. 키워드 목록
- `amenities`의 중요 키워드 목록 만들기
- 중복되는 키워드는 하나로 병합
<br>

3. 키워드 추출
- 기존 5757개에서 135개로 카테고리화
- 기존 정렬 순서를 유지하며 CATBOOST가 처리할 수 있는 text_features로 변환
- 기존 정렬이 호스트가 중요하다고 생각한 편의시설 중요 순서라고 판단
<br>

⭐️ 전처리 2 이후 성능 평가 결과

| 모델       | MAE     | MAPE    | RMSE     | R²      |
|------------|---------|---------|----------|---------|
| 기본 모델  | **70.595**  | 28.20%  | 311.9109 | 0.2826  |
| 전처리 01  | **37.0532** | 24.69%  | 53.8511  | 0.7133  |
| 전처리 02  | **36.9575** | 24.64%  | 53.7367  | 0.7145  |

--- 

## 📝 모델 성능 향상 전처리 3 _ MAPBOX API 활용
> 지하철, 버스정류장, 공항까지의 거리에 대한 컬럼 생성
<br>

1. 추가 데이터 수집
- 뉴욕은 대중교통 의존성 높은 도시
- 숙소와 대중교통을 연결
- MTA를 활용 대중교통 경/위도 수집
<br>

2. 숙소와 근접한 정류장 매칭
- 근접한 정류장을 찾을 때는 유클리드 거리 사용
- 해당 정류장과 숙소와 거리는 MAPBOX API상의 거리 계산 사용
  - 지하철, 버스정류장 : 도보거리
  - 공항 : 자동차거리
<br>

3. 새 컬럼 생성
- 각 숙소에서부터 가장 가까운 대중교통까지 거리 컬럼 생성 : `amenity_categories`
<br>

⭐️ 전처리 3 이후 성능 평가 결과
| 모델        | MAE     | MAPE    | RMSE     | R²      |
|-------------|---------|---------|----------|---------|
| 기본 모델   | **70.595**  | 28.20%  | 311.9109 | 0.2826  |
| 전처리 01   | **37.0532** | 24.69%  | 53.8511  | 0.7133  |
| 전처리 02   | **36.9575** | 24.64%  | 53.7367  | 0.7145  |
| 전처리 03   | **36.9382** | 24.67%  | 53.7553  | 0.7143  |

--- 

## 📝 모델 성능 향상 전처리 4 _ 하이퍼 파라미터 튜닝 값
> 여러 조합 실험을 통해 최적의 성능을 내는 파라미터 설정값 도출
<br>

| 하이퍼파라미터             | 값                                         | 설명                                      |
|---------------------------|--------------------------------------------|-------------------------------------------|
| `iterations`              | 3049                                       | 학습 반복 횟수                             |
| `depth`                   | 7                                          | 트리의 최대 깊이                           |
| `learning_rate`           | 0.016124356582299097                       | 학습률                                    |
| `l2_leaf_reg`             | 2                                          | L2 정규화 계수                             |
| `bagging_temperature`     | 0.5876999108727793                         | 다양성 조절 (샘플링 온도)                 |
| `border_count`            | 196                                        | 수치형 feature 이산화 경계 개수           |
| `cat_features`            | cat_features_idx                       | 범주형 feature                             |
| `text_features`           | amenity_categories                          | 텍스트 feature                             |
| `early_stopping_rounds`   | 100                                        | 학습 조기 종료 기준                        |
| `verbose`                 | 0                                          | 로그 출력 주기 (0이면 출력 없음)          |

---

## 📊 모델 성능 최종 비교 표

| 구분                  | MAE 📉     | MAPE 🧮   | MSE 🧾       | RMSE 📏    | R² 📈     |
|-----------------------|------------|----------|--------------|------------|-----------|
| 기본 모델 - Train     | 55.8042    | 23.82%   | 54539.5008   | 233.5369   | 0.7006    |
| 기본 모델 - Test      | 70.595     | 28.20%   | 97288.4228   | 311.9109   | 0.2826 |
| 전처리 01 - Train     | 30.4751    | 19.77%   | 2059.823     | 45.3853    | 0.7936    |
| 전처리 01 - Test      | 37.0532    | 24.69%   | 2899.9423    | 53.8511    | 0.7133 |
| 전처리 02 - Train     | 30.3174    | 19.65%   | 2041.0926    | 45.1785    | 0.7955    |
| 전처리 02 - Test      | 36.9575    | 24.64%   | 2887.6374    | 53.7367    | 0.7145 |
| 전처리 03 - Train     | 30.8385    | 20.16%   | 2081.5734    | 45.6243    | 0.7914    |
| 전처리 03 - Test      | 36.9382    | 24.67%   | 2889.6354    | 53.7553    | 0.7143 |
| 전처리 04 - Train     | **30.9174**    | **20.15%**   | 2101.1836    | 45.8387    | 0.7894    |
| 전처리 04 - Test      | **36.8096**    | **24.50%**   | 2878.8468    | 53.6549    | 0.7154 |

---

## 📣 STREAMLIT 활용 서비스 개발
> 모델 성능 확보 후, 실질적인 활용을 위해 서비스 개발 단계로 전환
<br>

- 도구 선택 (STREAMLIT 선택 이유)
  - END-TO-END 구현 가능 : 기획부터 시각화, 배포까지
  - 빠른 개발 속도 : 가볍고 신속한 프로토타입 제작
  - PYTHON 기반 : 분석-개발 간 자연스로운 연계
  - 간편한 웹 배포 : 누구나 접근 가능한 결과 공유
<br>

- 개발 : https://huggingface.co/spaces/DHDHDDHH/testml

---

## 💡 인사이트 도출 및 결론
- 호갱 방지
  - 내가 고른 숙소가 합리적 가격인지 즉시 확인 가능
- 스트레스 해소
  - 가격 검증 후 안심하고 선택 가능
- 만족도 향상
  - 여행 준비 과정의 불안 감소
- 정보 비대칭 해소
  - 스마트 요금제 등 숨겨진 요소 파악 도움
<br>

: 고객 신뢰 확보 -> 재방문 유도 -> 플랫폼 성장과 함께 신뢰 기반 커뮤니티 형성
